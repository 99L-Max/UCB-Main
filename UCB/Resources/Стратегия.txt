\c\ВВЕДЕНИЕ
Одно из сравнительно новых направлений в математике – управление в случайной среде. Например, когда из двух действий нужно выбрать лучшее, но при этом оно заранее неизвестно. Такая задача получила название «Задача о двуруком бандите», в которой вводится модель игрального автомата с двумя рукоятками. Обычно рассматривается случай, когда рукоятей две, но в дипломной работе рассмотрена модификация задачи при числе вариантов больше двух.
Задача имеет несколько применений. Например, в экономике её можно использовать для оптимизации инвестиций, в маркетинге – для оптимизации распределения рекламного бюджета между различными каналами продвижения продукта. Также задача может быть применена для поиска оптимальной стратегии в сложных системах, таких как компьютерные игры или автономные роботы.
Для выбора лучшей рукояти предлагается использовать стратегию UCB: выбирать рукоять, которой соответствует наибольшая верхняя граница доверительного интервала среднего дохода. Преимущество данной стратегии является её способность к быстрому обучению, так как она позволяет игроку находить лучшее решение достаточно быстро. Алгоритм наиболее эффективен, когда нужно быстро выбрать наилучшее действие, используя ограниченные ресурсы.
Целью дипломной работы является рассмотреть модификации стратегии UCB на примере задачи о многоруком бандите.
\c\ДВУРУКИЙ БАНДИТ
В задачи в качестве математической модели рассматривается игральный автомат с двумя рукоятями [1, 2] под названием «двурукий бандит». Каждая рукоять имеет фиксированное и неизвестное математическое ожидание mi. При выборе рукояти один раз игрок получает одношаговый доход xi, а всего он может выбирать рукояти N раз, где N – горизонт управления. 
Цель игрока состоит в максимизации выигрыша, то есть математического ожидания полного дохода. Для этого в процессе игры необходимо определить рукоять, соответствующую наибольшему доходу, и обеспечить её преимущественный выбор. Поскольку mi неизвестны игрок вынужден решать дилемму «информация или управление». Суть дилеммы в том, что для получения наибольшего дохода хотелось бы выбирать ту рукоять, которой соответствует наибольший ожидаемый одношаговый доход xi, однако для определения лучшей рукояти требуется сравнивать её с остальными, применение которых ведет к уменьшению полного дохода.
В данной модели выбор рукояти означает выбор действия, поэтому в дальнейшем будем так их и называть.
\c\РАСПРЕДЕЛЕНИЕ ГАУССА
Рассмотрим гауссовского многорукого бандита с J≥2 действиями. Значение xi в момент времени n зависит только от выбранного действия yn и является нормально распределенной случайной величиной с плотностью
\c\f(x│mi)=exp⁡(-(x-mi)^2/(2Di))/√(2πDi)
при yn=i,i=1,...,J. Дисперсии D1,...,DJ известны и упорядочены по убыванию (D1≥D2≥...≥DJ). В дальнейшем данное требование можно отменить, так как алгоритм мало чувствителен к значительному изменению дисперсии, поэтому их можно оценить на начальном этапе управления.  Математические ожидания m1,...,mJ, напротив, неизвестны и расположены в произвольном порядке.
\c\РАСПРЕДЕЛЕНИЕ БЕРНУЛЛИ
Одно из возможных применений данной задачи: обработка данных. Но при слишком большом числе данных обрабатывать их по одному слишком долго. Поэтому рассмотрим модификацию задачи, когда автомат имеет распределение Бернулли.
\c\P(xi=1|yn=i)=pi, P(xi=0|yn=i)=1-pi=qi
То есть при выборе i-го действия xi=1 с вероятностью pi или xi=0 с вероятностью qi. При этом дисперсия D=pi*qi, если текущее выбранное действие yn=i [2, 3, 4]. 
Здесь 1 и 0 соответствуют успешной и неуспешной обработки одного данного. При этом данные обрабатывается пакетами, а доходы суммируются и в силу центральной предельной теоремы (ЦПТ) доходы будут близки к гауссовским. Пакетная обработка является удобной стратегией управления, поскольку она позволяет реже менять действие и время полной обработки будет определяться числом пакетов. Отметим, что пакетная версия также рассматривалась в [5, 6, 7] и порядок минимаксного риска равен N^(1⁄2) или близок к N^(1⁄2). Как было рассмотрено в [8], пусть данные разбиты на K пакетов размерностью M каждый. Тогда горизонт управления N=MK.
\c\ГЕНЕРАЦИЯ ДОХОДОВ
Для вычисления одношагового дохода xi бандита с распределением Гаусса используется случайная величина εi, имеющее стандартное нормальное распределение. Для её генерации используется ЦПТ.
\c\εi=∑sj-6
где j от 1 до 12 и sj - случайная величина, равномерно распределённая на отрезке [0; 1].
Для распределения Бернулли всё проще. С вероятностью pi генерируется 1 или с вероятностью qi генерируется 0, а затем одношаговые доходы суммируются.
\c\ВЫБОР ЛУЧШЕГО ДЕЙСТВИЯ
Поскольку доходы рукоятей заранее неизвестны, для начала нужно набрать статистику: по очереди выбрать все действия по одному разу и запомнить их доходы. Обозначим через Xi(n) текущий полный доход (сумма одношаговых доходов) i-й рукояти и пусть она была выбрана ni раз. Тогда (Xi(n))⁄ni   является точечной оценкой математического ожидания mi  . Может показаться логичным выбирать то действие, для которого средний доход наибольший, однако на начальном этапе [9] худшее действие может получить большую оценку.
\c\ВЫБОР ЛУЧШЕГО ДЕЙСТВИЯ ПРИ РАСПРЕДЕЛЕНИИ ГАУССА
Вместо точечных оценок математических ожиданий будем использовать верхние границы их интервальных оценок [10]. 
\c\Ui(n)=(Xi(n))/ni + ai*√(Di*ln(⁡n)/ni),
где ai>0 – параметры стратегии, n=n1+n2+⋯+nJ – общее количество выбранных действий. В таком случае лучшему действию будет соответствовать наибольшая Ui(n). Данная стратегия называется UCB (upper confidence bound – верхняя граница доверительного интервала). Она достаточно простая в применении и рассматривалась в [9, 11, 12, 13].
\c\ВЫБОР ЛУЧШЕГО ДЕЙСТВИЯ ПРИ РАСПРЕДЕЛЕНИИ БЕРНУЛЛИ
Для распределения Бернулли Ui(n) можно считать по формуле, но с поправкой, что значению ni будет соответствовать количество обработанных данных при выборе i-го действия, а n – общее количество обработанных данных. Однако также можно считать через количество обработанных пакетов
\c\(Ui (k)=(Xi(k))/ki + ai*√(Di*M*ln(⁡k)/ki),
где ki – число обработанных пакетов при выборе i-го действия, k=k1+k2+⋯+kJ – общее число обработанных пакетов. При этом дисперсия Di будет характеризовать разброс значений для пакета, поэтому её необходимо умножить на размерность M.
\c\ОЦЕНКА ДИСПЕРСИЙ
В условиях неопределённости, помимо математических ожиданий mi, также могут быть неизвестны и дисперсии Di. В таком случае перед вычислением Ui(n) необходимо сделать оценку дисперсий. Для получения несмещённой оценки дисперсии будем использовать формулу
\c\Di=(Xi(n)(ni-Xi(n)))/(ni(ni-1))
Данная формула применимы только к пакетной обработке данных, так как необходимо минимум два одношаговых дохода для оценки дисперсии, а для распределения Гаусса после начального этапа все ni=1.
\c\ФУНКЦИЯ ПОТЕРЬ
Основная задача стратегии UCB – подбор оптимального значения параметра ai так, чтобы максимальный проигрыш (потери) был минимальным. Данный подход называется минимаксный, так как он достаточно осторожный в принятии решения и учитывает все возможные риски, он рассматривался, например, в [10, 14]. Для вычисления потерь нужно сравнить максимальный возможный выигрыш с фактическим. Максимальный доход можно получить в том случае, если знать лучшее действие и всё время выбирать только его. Фактический доход – сумма полученных доходов, но важно отметить, что он является случайной величиной. Тогда потери можно считать, как нормированную разность максимального дохода и математического ожидания полного дохода.
\c\l(d)=(N*max⁡{m1,m2,...mJ}-E⁡(X1+X2+...+XJ))/√(DN),
где D=max⁡{D1,...,DJ}, m1=m+d(D⁄N)^(1⁄2), ml=m-d(D⁄N)^(1⁄2), l=2,...,J, d≥0 – отклонение. В случае распределения Гаусса будем рассматривать стандартное нормальное распределение при m=0,D=1. Для распределения Бернулли m=p, где pϵ[0;1], а D=0,25 – максимальная дисперсия одношагового дохода, которая достигается при p=0,5.
Ограничимся таким множеством параметров, так как оно описывает «близкие» распределения – именно на нём потери достигают максимального значения.
Если математические ожидания mi достаточно близкие и отличаются на очень малую величину δ, то для определения лучшего действия нужно больше времени и вследствие этого потери растут, что можно легко увидеть на графике.
На оси абсцисс откладываются отклонения d=δ(N⁄D)^(1⁄2), на оси ординат – значения потерь ln(d). Видно, что при слишком больших d потери стремятся к бесконечности. Это связано с тем, что при очень большом отклонении лучшее и худшее действия определяются сразу же на начальном этапе, но именно этот этап и увеличивает потери.
\c\Ограничение для распределения Бернулли
Для распределения Бернулли математическое ожидание p есть вероятность, которая, разумеется, принимает значение от 0 до 1. Следовательно, отклонение d не превосходит некоторой положительной величины.
\c\0≤d≤min⁡{p,q}(N⁄D)^(1⁄2)
\c\ЛИТЕРАТУРА
1. Пресман Э.Л., Сонин И. М. Последовательное управление по неполным данным. М.: Наука, 1982.
2. Berry D.A., Fristedt B. Bandit Problems: Sequential Allocation of Experiments. London, New York: Chapman and Hall, 1985.
3. Варшавский В.И. Коллективное поведение автоматов. М.: Наука, 1973
4. Цетлин М.Л. Исследования по теории автоматов и моделированию биологических систем. М.: Наука, 1969.
5. Колногоров А.В. Робастное параллельное управление в случайной среде и оптимизация обработки данных // Автоматика и телемеханика. 2014. № 12. С. 42–55.
6. Kolnogorov, A.V., A Gaussian two-armed bandit: A limit description, Probl. Peredachi Inf., 2020, vol. 56, no. 3, pp. 86–111.
7. Perchet, V., Rigollet, P., Chassang, S., and Snowberg, E., Batched bandit problems, Ann. Statist., 2016, vol. 44, no. 2, pp. 660–681.
8. Гарбарь С.В., Колногоров А.В. Адаптация стратегии UCB Дж. Басера для гауссовского многорукого бандита // Математическая Теория Игр и ее Приложения. 2022. Т.14. № 2. C. 3–30.
9. Lai T. L. Adaptive Treatment Allocation and the Multi-Armed Bandit Problem // Annals of Statist. 1987. V. 25. P. 1091–1114.
10. Bather J.A. The Minimax Risk for the Two-Armed Bandit Problem // Mathematical Learning Models – Theory and Algorithms. Lecture Notes in Statistics. New York Inc.: Springer-Verlag. V. 20. P. 1–11, 1983.
11. Auer, P., Using confidence bounds for exploitation-exploration trade-offs, J. Mach. Learn. Res., 2002, vol. 3, pp. 397–422.
12. Auer, P., Cesa-Bianchi, N., and Fischer, P., Finite-time analysis of the multi-armed bandit problem, Mach. Learn., 2002, vol. 47. N 2–3, pp. 235–256.
13. Lugosi, G. and Cesa-Bianchi, N., Prediction, Learning and Games, Cambridge: Cambridge University Press, 2006.
14. Vogel, W., An asymptotic minimax theorem for the two-armed bandit problem, Ann. Math. Stat., 1960, vol. 31, pp. 444–451.
